---
title: "Homework 3"
author: Yimiao Pang
data: October 17, 2021
output: github_document
---

```{r setup, message=FALSE}
library(p8105.datasets)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(patchwork)
```


## Problem 1

```{r 1.1, message=FALSE, collapse=TRUE}
# load data

data("instacart")

num_row = nrow(instacart)

# number of aisles & most ordered from

num_aisles = nlevels(factor(instacart$aisle))
sum_aisles = as.data.frame(table(factor(instacart$aisle)))
most_aisle = sum_aisles[which.max(sum_aisles$Freq), "Var1"]

```

In the instacard dataset, there are `r num_row` rows and 15 columns which include order_id, product_id, add_to_cart_order, reordered, user_id, eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order, product_name, aisle_id, department_id, aisle, and department. 

There are `r num_aisles` different aisles. The aisle most ordered from is `r most_aisle` aisle.

```{r 1.2, message=FALSE, collapse=TRUE}
# plot - number of items

instacart %>% 
  group_by(aisle, department) %>% 
  summarize(n_freq = n()) %>% 
  filter(n_freq > 10000) %>% 
  ggplot(aes(x = reorder(aisle, n_freq), y = n_freq, fill = department)) + geom_col() +
  labs(
    title = "number of items ordered in each aisle",
    x = "aisle",
    y = "number of orders"
  ) + 
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 1),
        axis.text = element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "bottom") + 
  coord_flip()

```

The x-axis represents the number of orders and the y-axis lists all the aisles whose orders were over 10000. There are 39 aisles from which over 10000 items were ordered. And we can see that the fresh vegetables is the aisle most frequently ordered from.

```{r 1.3, message=FALSE, collapse=TRUE}
# 3 most popular items
## baking ingredients
b_df = filter(instacart, aisle == "baking ingredients")
b_sum = as.data.frame(table(factor(b_df$product_name)))
b_3 = top_n(b_sum, 3, Freq)
b_tb = tibble(
  b_3[1],
  b_3[2]
) %>% 
  janitor::clean_names() %>% 
  rename(product_name = var1) %>% 
  mutate(aisle = "baking ingredients")

## dog food care
d_df = filter(instacart, aisle == "dog food care")
d_sum = as.data.frame(table(factor(d_df$product_name)))
d_3 = top_n(d_sum, 3, Freq)
d_tb = tibble(
  d_3[1],
  d_3[2]
) %>% 
  janitor::clean_names() %>% 
  rename(product_name = var1) %>% 
  mutate(aisle = "dog food care")

## packaged vegetables fruits
p_df = filter(instacart, aisle == "packaged vegetables fruits")
p_sum = as.data.frame(table(factor(p_df$product_name)))
p_3 = top_n(p_sum, 3, Freq)
p_tb = tibble(
  p_3[1],
  p_3[2]
) %>% 
  janitor::clean_names() %>% 
  rename(product_name = var1) %>% 
  mutate(aisle = "packaged vegetables fruits")

## combine

tb = rbind(b_tb, d_tb, p_tb) %>% 
  arrange(aisle, desc(freq))
tb = tb[c("aisle", "product_name", "freq")]
tb
```

```{r 1.4, message=FALSE, collapse=TRUE}
app_ice_df = filter(instacart, product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = order_hour_of_day,
    values_fn = list(order_hour_of_day = mean)
  )
 app_ice_df = app_ice_df[c("product_name", "0", "1", "2", "3", "4", "5", "6")] %>% 
  rename("Sunday" = "0",
         "Monday" = "1",
         "Tuesday" = "2",
         "Wednesday" = "3",
         "Thursday" = "4",
         "Friday" = "5",
         "Saturday" = "6")
app_ice_df
```


## Problem 2

```{r 2.1, message=FALSE, collapse=TRUE}
# load data
data("brfss_smart2010")

# data cleaning
df2 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response_rank = recode(response, 'Poor' = 0, 'Fair' = 1, 'Good' = 2, 'Very good' = 3, 'Excellent' = 4)) %>% 
  arrange(response_rank) %>% 
  select(-response_rank)

```

```{r 2.2, message=FALSE, collapse=TRUE}
# 2002 --- >=7
df2002 = filter(df2, year == 2002) %>% 
  select(locationabbr, locationdesc)

levels2002 = levels(factor(df2002$locationabbr))

i = 1
n = 0
states2002 = c()
while (i <= length(levels2002)) {
  state_df = df2002 %>% 
    filter(locationabbr == levels2002[i])
  if (nlevels(factor(state_df$locationdesc)) >= 7) {
    n = n + 1
    states2002[n] = levels2002[i]
  }
  i = i + 1
}

# 2010 --- >=7
df2010 = filter(df2, year == 2010) %>%
  select(locationabbr, locationdesc)

levels2010 = levels(factor(df2010$locationabbr))

i = 1
n = 0
states2010 = c()
while (i <= length(levels2010)) {
  state_df = df2010 %>%
    filter(locationabbr == levels2010[i])
  if (nlevels(factor(state_df$locationdesc)) >= 7) {
    n = n + 1
    states2010[n] = levels2010[i]
  }
  i = i + 1
}

states2002
states2010
```

In 2002, there are `r length(states2002)` states observed at 7 or more locations. They are `r states2002`.

In 2010, there are `r length(states2010)` states observed at 7 or more locations. They are `r states2010`.

```{r 2.3, message=FALSE, collapse=TRUE}
# extract data
ex_df = filter(df2, response == "Excellent") %>% 
  select(year, locationabbr, data_value)


# calculate averages & aggregate data frame
ex_ag_df = ex_df %>% 
  group_by(locationabbr, year) %>% 
  summarize(mean_data_value = mean(data_value, na.rm = TRUE))

# spaghetti plot
ggplot(ex_ag_df, aes(x = year, y = mean_data_value, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "right")
  labs(title = "average values over time within states",
       y = "average value")
```

```{r 2.4, message = FALSE, collapse = TRUE}
df2 %>% 
  group_by(year, locationabbr) %>% 
  filter((year == 2006 | year == 2010),
         locationabbr == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) + 
  geom_boxplot() +
  labs(title = "Distribution of data_value for responses in NY State (2006 and 2010)") +
  facet_grid(. ~year)
```


## Problem 3

```{r 3.1, message=FALSE, collapse=TRUE}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekday_vs_weekend = recode(day,
                                     "Saturday" = "weekend",
                                     "Sunday" = "weekend",
                                     "Monday" = "weekday",
                                     "Tuesday" = "weekday",
                                     "Wednesday" = "weekday", 
                                     "Thursday" = "weekday",
                                     "Friday" = "weekday")) %>% 
  select(week, day_id, day, weekday_vs_weekend, everything()) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(minute = as.numeric(minute))

accel_df
```

There are 50400 observations and 6 different variables included in the dataframe. The variables include week, day_id, day, weekday_vs_weekend, minute and activity_count.

```{r 3.2, message=FALSE, collapse=TRUE}
accel_df %>% 
  group_by(week, day, day_id) %>%
  summarize(total = sum(activity_count)) %>% 
  knitr::kable()
```

According the the table above, the total activity count is more likely to increase from Monday to Friday and to decrease during the weekend. 

```{r 3.3, message=FALSE, collapse=TRUE}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) + 
  geom_line(alpha = 0.8) +
  labs(
    title = "24-hour activity for each day",
    x = "time",
    y = "activity count"
  ) +
  scale_x_continuous(breaks = c(0, 180, 360, 540, 720, 900, 1080, 1260, 1440),
                     labels = c("12am", "3am", "6am", "9am", "12pm", "3pm", "6pm", "9pm", "12am"),
                     limits = c(0, 1440))
```

Within a day, the activity count is lower in the midnight, and higher during the daytime and extremely high during 7pm-10pm. After 10pm, it drops dramatically to a low level. There are 4 peaks in a day, one at 7am, one at 11am, one at 5pm and the last one is at 9pm. 

Moreover, we can see that the activity count is higher at Friday between 8pm and 10pm than other days.